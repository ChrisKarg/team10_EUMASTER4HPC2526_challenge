# Ollama Service Configuration
service:
  name: ollama
  description: Ollama LLM inference service
  container_image: ollama_latest.sif
  command: ollama
  args: ["serve"]

  # Container configuration
  container:
    # Docker source for automatic building if image doesn't exist
    docker_source: "docker://ollama/ollama:latest"
    # Path where container image is stored/should be downloaded
    image_path: "$HOME/containers/ollama_latest.sif"

  # Resource requirements (SLURM job configuration)
  # Here specify what options to include in SLURM script (sbatch)
  resources:
    time: "00:30:00"
    qos: default
    partition: gpu
    nodes: 1
    ntasks: 1
    ntasks_per_node: 1
    gres: "gpu:1"         # Finer control over GPU allocation
    mem: "16GB"           # Fine control over memory allocation

  # Environment variables for container networking
  environment:
    OLLAMA_TLS_SKIP_VERIFY: "1"
    OLLAMA_HOST: "0.0.0.0:11434"  # Bind to all interfaces
    OLLAMA_KEEP_ALIVE: "5m"       # Keep model loaded
    CUDA_VISIBLE_DEVICES: "0"
    
  # Exposed ports (accessible from other containers on same node)
  ports:
    - 11434

  # Health check configuration
  health_check:
    endpoint: "http://localhost:11434/api/tags"
    timeout: 30
    retries: 5
    interval: 10
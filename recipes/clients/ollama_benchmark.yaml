# Ollama Benchmark Client Configuration
client:
  name: ollama_benchmark
  description: Benchmark client for Ollama LLM service
  container_image: benchmark_client.sif
  duration: 300  # 5 minutes

  # Resource requirements (SLURM job configuration)
  resources:
    partition: gpu  # Same partition as service for better connectivity
    time: "00:30:00"
    qos: default
    nodes: 1
    ntasks: 1
    ntasks_per_node: 1
    mem: "4GB"

  # Environment variables
  environment:
    OLLAMA_TLS_SKIP_VERIFY: "1"
    BENCHMARK_DURATION: "300"
    BENCHMARK_TYPE: "ollama"

  # Benchmark parameters - endpoint will be auto-resolved by orchestrator
  parameters:
    # endpoint: auto-resolved from target_service configuration
    model: "llama2"
    num_requests: 10      # Reduced for testing
    concurrent_requests: 2 # Reduced for testing  
    prompt_length: 50     # Reduced for testing
    max_tokens: 100       # Reduced for testing
    output_file: "/tmp/ollama_benchmark_results.json"
    wait_for_service: 120  # Wait longer for service to be ready

  # Target service configuration (used by orchestrator for service discovery)
  target_service:
    name: ollama
    port: 11434
    health_check_endpoint: "/api/tags"

  # Script configuration
  script:
    name: "ollama_benchmark.py"           # Name of the script to run
    local_path: "benchmark_scripts/"     # Where to find script locally
    remote_path: "$HOME/benchmark_scripts/" # Where to upload script remotely